{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Digit Recognition**\n",
    "\n",
    "DSC-440 Project 2\n",
    "\n",
    "Brian Chaffee\n",
    "\n",
    "---\n",
    "**Description:**\n",
    "\n",
    "Using the MNIST dataset adapted from the Kaggle Competition. The objective is to create a Convolutional Neural Network that successfuly is able to predict the correct digit from 0 to 9 from a list of handwritten digits. \n",
    "\n",
    "I will be utilizing data augmentation alongside the convoltutional neural network in order to better prevent overfitting the created model.\n",
    "\n",
    "[Kaggle Link](https://www.kaggle.com/competitions/digit-recognizer/code?competitionId=3004&sortBy=voteCount&excludeNonAccessedDatasources=true)\n",
    "- The only adaptation is using `tensorflow.keras.datasets` instead of the provided datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "---\n",
    "\n",
    "Utilizing the Keras package from Tensorflow due to the ease and intuitive aspects of creating the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# # Basic Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Machine Learning\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "\n",
    "# # # Preprocessing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # # Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# # Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "---\n",
    "\n",
    "Utilizing the tensorflow library to load in the dataset from it's built in `load_data` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Loading dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at a small sample of the images in the dataset to vizualize what the handwritten digit images look like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example images\n",
    "num_images = 5\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(1, num_images, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(y_train[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure displaying the count of each digit to verify that they are equally distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(y_train, bins=range(11), align='left', rwidth=0.8, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram of Labels in MNIST Training Set')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(range(10))\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "The data needs to be normalized and reshaped in order to fit within the CNN structure that Keras accepts\n",
    "\n",
    "The shape of the training and testing sets help determine preprocessing methods in order to fit within the model. Here, the shape shows that there are 60000 training imagees and there are a 28 pixels that represent the height and width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train images shape:\", X_train.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "print(\"Test images shape:\", X_test.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing\n",
    "\n",
    "Normalizing the data basically grayscales the image and reduces the extra illumination around the edges. The data is divided by 255 because it reduces the color channels of the pixels and also reduces the effects that larger numbers may have on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping\n",
    "\n",
    "The data also needs to be reshaped in order to be fit within a CNN model. The CNN requires an additional channel that represents the color of the image. Here it is represented as a one in order to describe the grayscale images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "---\n",
    "\n",
    "### Training and Validation Split\n",
    "Creating the Convolutional Neural Network, the data needs to be split into training and testing (validation) sets. Here, 20 percent of the dataset is sectioned out in order to be used as a validation set for the model. \n",
    "\n",
    "*However, the dataset is already split into 60000 training points and 10000 testing points from the `load_data` package in keras.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data Augmentation is going to help reduce the possibility of overfitting the model by artifically increasing the amount of training data. Using the `ImageDataGenerator` from keras, a selection of changes are able to be input where the `X_train` data will be altered slightly when input into the model fitting.\n",
    "\n",
    "The data is transformed slightly in order to simulate the variability in handwriting and increase the training set in order to reduce overfitting data. It is able to see multiple variations on the digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "\n",
    "    rotation_range = 15,        # rotate image randomly within range of 10 degrees\n",
    "    width_shift_range = 0.1,    # shift image horizontally\n",
    "    height_shift_range = 0.1,   # shift image vertically\n",
    "    zoom_range = 0.1            # zooms image\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data could be flipped vertically and horizontally, however, some numbers like 6 and 9 look like each other when flipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model\n",
    "\n",
    "The Sequential function allows adding one layer at a time starting at input and working in order till the output layer. (Using Sequential, it recommends having an intial input layer rather in specifying input shape within the first layer).\n",
    "\n",
    "The model follows an initial convolutional layer that takes 112 filters\n",
    "\n",
    "`Conv2D()` layer\n",
    "\n",
    "`AveragePooling2D()` layer\n",
    "\n",
    "`Flatten()` layer\n",
    "\n",
    "`Dense()` layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "\n",
    "    layers.Input(shape=(28,28,1)),\n",
    "\n",
    "    layers.Conv2D(112, kernel_size = (7, 7), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Dropout((0.25)),\n",
    "\n",
    "    layers.Conv2D(49, kernel_size = (3, 3), activation='relu'),\n",
    "    layers.Conv2D(49, kernel_size = (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "\n",
    "    layers.Flatten(),\n",
    "\n",
    "    layers.Dense(56, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store altered variabels\n",
    "epochs = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model_history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size), epochs=epochs, validation_data = (X_test,y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "#test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "#print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Training and Testing Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = range(1, len(model_history.history['accuracy']) + 1)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10, 6), layout='constrained')\n",
    "ax[0].plot(epoch_range, model_history.history['accuracy'], 'b-', label='Training Accuracy')\n",
    "ax[0].plot(epoch_range, model_history.history['val_accuracy'], 'r-', label='Validation Accuracy')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[0].set_title('Training vs. Validation Curve', fontweight='bold')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "ax[1].plot(epoch_range, model_history.history['loss'], 'b', label='Training Loss')\n",
    "ax[1].plot(epoch_range, model_history.history['val_loss'], 'r', label='Validation Loss')\n",
    "ax[1].set_ylabel('Loss')\n",
    "ax[1].set_title('Training vs Validation Loss', fontweight='bold')\n",
    "ax[1].legend(loc='best')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix for Digit MNIST Classification')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Citations\n",
    "\n",
    "1. LeCun, Y., Cortes, C., & Burges, C. J. (2010). MNIST handwritten digit database. ATT Labs [Online].  Available: Http://Yann. Lecun. Com/Exdb/Mnist, 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
